{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T08:58:40.001178Z",
     "start_time": "2019-03-31T08:58:27.918446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats.stats import pearsonr\n",
    "from Progbar import Progbar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "if not os.path.exists('./filters'):\n",
    "    os.mkdir('./filters')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 100, 50)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "def add_noise(img):\n",
    "    noise = torch.randn(img.size()) * 0.2\n",
    "    noisy_img = img + noise\n",
    "    return noisy_img\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 100, 50)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    if max_tensor != 0:\n",
    "        tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor.float())\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1))\n",
    "   # transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "\n",
    "class SimulatedDataset(Dataset):\n",
    "    '''\n",
    "    每一个 Item 是 (5000, ) 的向量\n",
    "    '''\n",
    "\n",
    "    def __init__(self, simulated_csv_data_path, true_csv_data_path, transform=None):\n",
    "        self.simulated_csv_data = pd.read_csv(simulated_csv_data_path)\n",
    "        self.true_csv_data_path = pd.read_csv(true_csv_data_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        print(\"len : \", len(self.simulated_csv_data.index))\n",
    "        return len(self.simulated_csv_data.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a_column_of_simulated_data = self.simulated_csv_data.iloc[index:index+1,1:]\n",
    "        a_column_of_true_data = self.true_csv_data_path.iloc[index:index+1,1:]\n",
    "        a_column_of_simulated_data = np.asarray(a_column_of_simulated_data).reshape(1,-1)\n",
    "        a_column_of_true_data = np.asarray(a_column_of_true_data).reshape(1,-1)\n",
    "        if self.transform is not None:\n",
    "            a_column_of_simulated_data = self.transform(a_column_of_simulated_data)\n",
    "            a_column_of_true_data = self.transform(a_column_of_true_data)\n",
    "        simulated_true_pack = (a_column_of_simulated_data, a_column_of_true_data)\n",
    "        return simulated_true_pack\n",
    "\n",
    "dataset = SimulatedDataset(simulated_csv_data_path=\"./data/counts_simulated_dataset1_dropout0.05.csv\",\n",
    "                           true_csv_data_path=\"./data/true_counts_simulated_dataset1_dropout0.05.csv\",\n",
    "                           transform=img_transform)\n",
    "# dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2000, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 2000),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AutoEncoder().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T09:06:46.094197Z",
     "start_time": "2019-03-31T08:59:03.400910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  5000\n",
      "len :  5000\n",
      " 221/5000 [>.............................] - ETA: 525s - loss: 0.060552 - MSE_loss: 0.017952 - p-value: 0.721174 - PCC: nan  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxy/pyworks/env35/lib/python3.5/site-packages/scipy/stats/stats.py:3038: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 462s - loss: 0.008033 - MSE_loss: 0.001414 - p-value: 0.728222 - PCC: nan   - ETA: 6s - loss: 0.008065 - MSE_loss: 0.001424 - p-value: 0.728726 - ETA: 5s - loss: 0.00\n",
      "epoch [1/20], loss:0.0035, MSE_loss:0.0005, PCC:0.0451, p-value:0.0435\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 100, 50]' is invalid for input of size 2000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-07e162a3a9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m           .format(epoch + 1, num_epochs, loss.data, MSE_loss.data, PCC, p_value))\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-3179b4b8593a>\u001b[0m in \u001b[0;36mto_img\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 100, 50]' is invalid for input of size 2000"
     ]
    }
   ],
   "source": [
    "#if os.path.exists('./sim_autoencoder.pth'):\n",
    "    #model.load_state_dict(torch.load('./sim_autoencoder.pth'))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    prog = Progbar(len(dataloader))\n",
    "    for i, data in enumerate(dataloader):\n",
    "        (noisy_data, true_data) = data\n",
    "        noisy_data = Variable(noisy_data).float().to(device)\n",
    "        true_data = Variable(true_data).float().to(device)\n",
    "        # ===================forward=====================\n",
    "        # print(true_data.size())\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, true_data)\n",
    "        MSE_loss = nn.MSELoss()(output, true_data)\n",
    "        np1 = output.cpu().detach().numpy().reshape(-1)\n",
    "        np2 = true_data.cpu().detach().numpy().reshape(-1)\n",
    "        PCC, p_value = pearsonr(np1, np2)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # =====================log=======================\n",
    "        prog.update(i + 1, [(\"loss\", loss.item()), (\"MSE_loss\", MSE_loss.data), (\"PCC\", PCC), (\"p-value\", p_value)])\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}, PCC:{:.4f}, p-value:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data, MSE_loss.data, PCC, p_value))\n",
    "    if epoch % 10 == 0:\n",
    "        x = to_img(true_data.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        x_noisy = to_img(noisy_data.cpu().data)\n",
    "        weights = to_img(model.encoder[0].weight.cpu().data)\n",
    "        save_image(x, './mlp_img/x_{}.png'.format(epoch))\n",
    "        save_image(x_hat, './mlp_img/x_hat_{}.png'.format(epoch))\n",
    "        save_image(x_noisy, './mlp_img/x_noisy_{}.png'.format(epoch))\n",
    "        save_image(weights, './filters/epoch_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './512BCE_2000_sim_autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T07:33:32.017214Z",
     "start_time": "2019-03-31T07:33:31.897635Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(simulated_csv_data, filename=\"./image.png\"):\n",
    "    aaa = np.asarray(simulated_csv_data.iloc[:, 1:])\n",
    "    aaa=torch.Tensor(aaa).float()\n",
    "    aaa=aaa.view(1, 1, aaa.shape[0], aaa.shape[1])\n",
    "    save_image(aaa, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T07:35:33.313983Z",
     "start_time": "2019-03-31T07:35:32.484720Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('./512BCE_sim_autoencoder.pth'):\n",
    "    model.load_state_dict(torch.load('./512BCE_sim_autoencoder.pth', map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T08:55:16.067513Z",
     "start_time": "2019-03-31T08:55:14.988471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len :  5000\n",
      "torch.Size([1, 1, 1, 2000])\n",
      "tensor([[[[0.5077, 0.5086, 0.4863,  ..., 0.4904, 0.5015, 0.4818]]]]) torch.Size([1, 1, 1, 2000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "for i, data in enumerate(dataloader):\n",
    "    (noisy_data, true_data) = data\n",
    "    noisy_data = Variable(noisy_data).float().to(device)\n",
    "    true_data = Variable(true_data).float().to(device)\n",
    "    # ===================forward=====================\n",
    "    output = model(noisy_data)\n",
    "    d = output.cpu().data\n",
    "    print(d , d.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
